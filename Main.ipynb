{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf54b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j7srz8pvggt",
   "metadata": {},
   "source": [
    "Now we'll import the necessary libraries for web scraping, HTML parsing, and environment variable management:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7j6ku2miguu",
   "metadata": {},
   "source": [
    "# NPR News Web Scraper\n",
    "\n",
    "This notebook demonstrates web scraping of NPR news articles using the Decodo API and BeautifulSoup for HTML parsing.\n",
    "\n",
    "## Setup and Dependencies\n",
    "\n",
    "First, we'll install the required Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4610a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ry8adwafol",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "Load environment variables from a `.env` file to securely store the Decodo API authentication token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bffb21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "DECODO_AUTH = os.getenv('DECODO_AUTH_FIELD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h5itwwhzvze",
   "source": "Load the environment variables and authenticate with the Decodo API using the token stored in the `.env` file:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ke6o9nqh6ls",
   "metadata": {},
   "source": [
    "## Web Scraping Function\n",
    "\n",
    "Define a function to crawl URLs using the Decodo API. This function takes a URL and returns the scraped content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd42f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def crwal_url(URL_TO_CRWAL):  \n",
    "  url = \"https://scraper-api.decodo.com/v2/scrape\"\n",
    "    \n",
    "  payload = {\n",
    "        \"url\": URL_TO_CRWAL\n",
    "  }\n",
    "    \n",
    "  headers = {\n",
    "      \"accept\": \"application/json\",\n",
    "      \"content-type\": \"application/json\",\n",
    "      \"authorization\": DECODO_AUTH\n",
    "  }\n",
    "    \n",
    "  response = requests.post(url, json=payload, headers=headers)\n",
    "  \n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xt4doxrudji",
   "metadata": {},
   "source": [
    "## Target URLs Configuration\n",
    "\n",
    "Define the NPR news category URLs that we want to scrape. Each category has its own endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec96ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_to_crawl = {\n",
    "  \"Politics\" : \"https://www.npr.org/get/1014/render/partial/next\", \n",
    "  \"business\" : \"https://www.npr.org/get/1006/render/partial/next\",\n",
    "  \"Health\" : \"https://www.npr.org/get/1128/render/partial/next\", \n",
    "  \"Science\" : \"https://www.npr.org/get/1007/render/partial/next\",\n",
    "  \"Climate\" : \"https://www.npr.org/get/1167/render/partial/next\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ju4t8yy8s4i",
   "metadata": {},
   "source": [
    "## Scraping Politics Articles\n",
    "\n",
    "Test the scraper by crawling the Politics section with pagination parameters (start index and batch size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8558cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_url = urls_to_crawl[\"Politics\"]\n",
    "start_index = 1\n",
    "batch_size = 10\n",
    "crawled_url = crwal_url(f\"{category_url}?start={start_index}&count={batch_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xwmp1orjvs",
   "metadata": {},
   "source": [
    "## Processing the Response\n",
    "\n",
    "Parse the JSON response from the Decodo API to extract the scraped content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_url_json = json.loads(crawled_url.text)\n",
    "crawled_url_json['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ph12z9e5lg",
   "metadata": {},
   "source": [
    "## Extracting HTML Content\n",
    "\n",
    "Get the HTML content from the first result in the scraped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ee316",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string = crawled_url_json['results'][0]['content']\n",
    "html_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imkcjjna7y",
   "metadata": {},
   "source": [
    "## HTML Parsing with BeautifulSoup\n",
    "\n",
    "Parse the HTML content and extract article information. Here we find all article elements and extract the first anchor tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f16bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_string,'html.parser')\n",
    "for article in soup.find_all('article'):\n",
    "  anchor_tag  = article.find('a')\n",
    "  article_url = anchor_tag['href']\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36983f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(article_url):\n",
    "  try:\n",
    "    crawled_article = crwal_url(article_url)\n",
    "    crawled_article_json = json.loads(crawled_article.text)\n",
    "    if crawled_article_json['results'][0][\"status_code\"] != 200:\n",
    "      return None\n",
    "\n",
    "    html_string = crawled_article_json['results'][0]['content']\n",
    "    soup = BeautifulSoup(html_string,'html.parser')\n",
    "    story_div = soup.find('div', id='storytext')\n",
    "    if story_div is None:\n",
    "      return None\n",
    "\n",
    "    article_text = story_div.get_text(strip=True, separator='\\n')\n",
    "\n",
    "    return article_text\n",
    "  except:\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0yxyz7i5b",
   "source": "## Article Text Extraction Function\n\nDefine a function to extract the actual text content from individual article URLs. This function handles the full article scraping process:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d0f1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_article(category_url, batch_size = 10):\n",
    "  start_index = 1\n",
    "  while True:\n",
    "    crawled_page = crwal_url(f\"{category_url}?start={start_index}&count={batch_size}\")\n",
    "    crawled_page_json = json.loads(crawled_page.text)\n",
    "\n",
    "    if crawled_page_json['results'][0]['status_code'] != 200:\n",
    "      break\n",
    "\n",
    "    html_string = crawled_page_json['results'][0]['content']\n",
    "    soup = BeautifulSoup(html_string,'html.parser')\n",
    "\n",
    "\n",
    "    for article in soup.find_all('article'):\n",
    "      anchor_tag = article.find('a')\n",
    "      if anchor_tag is None:\n",
    "        continue\n",
    "      article_url = anchor_tag['href']\n",
    "      article_text = get_article_text(article_url)\n",
    "      if article_text is None:\n",
    "        continue\n",
    "\n",
    "      yield article_text\n",
    "    start_index += batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v0w782ptm6",
   "source": "## Article Iterator Function\n\nCreate a generator function that iterates through all articles in a category with pagination support:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed66e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling Politics\n",
      "Crawled 1 articles\n",
      "Crawled 2 articles\n",
      "Crawled 3 articles\n",
      "Crawled 4 articles\n",
      "Crawled 5 articles\n",
      "Crawling business\n",
      "Crawled 1 articles\n",
      "Crawled 2 articles\n",
      "Crawled 3 articles\n",
      "Crawled 4 articles\n",
      "Crawled 5 articles\n",
      "Crawling Health\n",
      "Crawled 1 articles\n",
      "Crawled 2 articles\n",
      "Crawled 3 articles\n",
      "Crawled 4 articles\n",
      "Crawled 5 articles\n",
      "Crawling Science\n",
      "Crawled 1 articles\n",
      "Crawled 2 articles\n",
      "Crawled 3 articles\n",
      "Crawled 4 articles\n",
      "Crawled 5 articles\n",
      "Crawling Climate\n",
      "Crawled 1 articles\n",
      "Crawled 2 articles\n",
      "Crawled 3 articles\n",
      "Crawled 4 articles\n",
      "Crawled 5 articles\n"
     ]
    }
   ],
   "source": [
    "data = [] \n",
    "for news_category, category_url in urls_to_crawl.items():\n",
    "  print(f\"Crawling {news_category}\")\n",
    "  article_crawled_num = 0\n",
    "  for article_text in get_next_article(category_url):\n",
    "    data.append({'news_categoty' : news_category, 'article' : article_text})\n",
    "    article_crawled_num += 1\n",
    "    print(f\"Crawled {article_crawled_num} articles\")\n",
    "    if article_crawled_num >= 5:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8lcg3vsptub",
   "source": "## Main Scraping Loop\n\nExecute the main scraping process across all news categories. \n\n**TODO: When you hit the API limit, create a new DECODO account and re-run these cells**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ef718b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('news_articles_Dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etqzc7zft8",
   "source": "## Save Data to CSV\n\nConvert the collected article data into a pandas DataFrame and save it as a CSV file:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5c3feca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_categoty</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Health</td>\n",
       "      <td>Florida's Surgeon General Dr. Joseph Ladapo at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Climate</td>\n",
       "      <td>Energy Secretary Chris Wright spearheaded a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Sen. Elizabeth Warren, D-Mass., speaks during ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Science</td>\n",
       "      <td>Three scientists learned they carry genes that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>business</td>\n",
       "      <td>\"Vanny,\" as this 2005 Chrysler Town &amp; Country ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_categoty                                            article\n",
       "13        Health  Florida's Surgeon General Dr. Joseph Ladapo at...\n",
       "20       Climate  Energy Secretary Chris Wright spearheaded a re...\n",
       "0       Politics  Sen. Elizabeth Warren, D-Mass., speaks during ...\n",
       "19       Science  Three scientists learned they carry genes that...\n",
       "7       business  \"Vanny,\" as this 2005 Chrysler Town & Country ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_csv = pd.read_csv('news_articles_Dataset.csv')\n",
    "head_csv.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i2bbxbed7gc",
   "source": "## Data Preview\n\nLoad and display a sample of the scraped data to verify the results:",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}