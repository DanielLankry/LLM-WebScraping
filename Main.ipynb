{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bf54b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\daniel\\appdata\\roaming\\python\\python313\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eba866",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets\n",
    "%pip install evaluate\n",
    "%pip install transformers\n",
    "%pip install transformers[torch]\n",
    "%pip install \"accelerate>=0.26.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e881431",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub[hf_extras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j7srz8pvggt",
   "metadata": {},
   "source": [
    "Now we'll import the necessary libraries for web scraping, HTML parsing, and environment variable management:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7j6ku2miguu",
   "metadata": {},
   "source": [
    "# NPR News Web Scraper\n",
    "\n",
    "This notebook demonstrates web scraping of NPR news articles using the Decodo API and BeautifulSoup for HTML parsing.\n",
    "\n",
    "## Setup and Dependencies\n",
    "\n",
    "First, we'll install the required Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4610a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import huggingface_hub\n",
    "import re\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ry8adwafol",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "Load environment variables from a `.env` file to securely store the Decodo API authentication token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bffb21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "DECODO_AUTH = os.getenv('DECODO_AUTH_FIELD')\n",
    "HF_LLM_TOKEN = os.getenv('LLM_MODEL_TOKEN_HF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h5itwwhzvze",
   "metadata": {},
   "source": [
    "Load the environment variables and authenticate with the Decodo API using the token stored in the `.env` file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ke6o9nqh6ls",
   "metadata": {},
   "source": [
    "## Web Scraping Function\n",
    "\n",
    "Define a function to crawl URLs using the Decodo API. This function takes a URL and returns the scraped content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd42f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def crwal_url(URL_TO_CRWAL):  \n",
    "  url = \"https://scraper-api.decodo.com/v2/scrape\"\n",
    "    \n",
    "  payload = {\n",
    "        \"url\": URL_TO_CRWAL\n",
    "  }\n",
    "    \n",
    "  headers = {\n",
    "      \"accept\": \"application/json\",\n",
    "      \"content-type\": \"application/json\",\n",
    "      \"authorization\": DECODO_AUTH\n",
    "  }\n",
    "    \n",
    "  response = requests.post(url, json=payload, headers=headers)\n",
    "  \n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xt4doxrudji",
   "metadata": {},
   "source": [
    "## Target URLs Configuration\n",
    "\n",
    "Define the NPR news category URLs that we want to scrape. Each category has its own endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec96ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_to_crawl = {\n",
    "  \"Politics\" : \"https://www.npr.org/get/1014/render/partial/next\", \n",
    "  \"business\" : \"https://www.npr.org/get/1006/render/partial/next\",\n",
    "  \"Health\" : \"https://www.npr.org/get/1128/render/partial/next\", \n",
    "  \"Science\" : \"https://www.npr.org/get/1007/render/partial/next\",\n",
    "  \"Climate\" : \"https://www.npr.org/get/1167/render/partial/next\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ju4t8yy8s4i",
   "metadata": {},
   "source": [
    "## Scraping Politics Articles\n",
    "\n",
    "Test the scraper by crawling the Politics section with pagination parameters (start index and batch size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_url = urls_to_crawl[\"Politics\"]\n",
    "start_index = 1\n",
    "batch_size = 10\n",
    "crawled_url = crwal_url(f\"{category_url}?start={start_index}&count={batch_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xwmp1orjvs",
   "metadata": {},
   "source": [
    "## Processing the Response\n",
    "\n",
    "Parse the JSON response from the Decodo API to extract the scraped content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_url_json = json.loads(crawled_url.text)\n",
    "crawled_url_json['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ph12z9e5lg",
   "metadata": {},
   "source": [
    "## Extracting HTML Content\n",
    "\n",
    "Get the HTML content from the first result in the scraped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ee316",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string = crawled_url_json['results'][0]['content']\n",
    "html_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imkcjjna7y",
   "metadata": {},
   "source": [
    "## HTML Parsing with BeautifulSoup\n",
    "\n",
    "Parse the HTML content and extract article information. Here we find all article elements and extract the first anchor tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f16bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_string,'html.parser')\n",
    "for article in soup.find_all('article'):\n",
    "  anchor_tag  = article.find('a')\n",
    "  article_url = anchor_tag['href']\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36983f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(article_url):\n",
    "  try:\n",
    "    crawled_article = crwal_url(article_url)\n",
    "    crawled_article_json = json.loads(crawled_article.text)\n",
    "    if crawled_article_json['results'][0][\"status_code\"] != 200:\n",
    "      return None\n",
    "\n",
    "    html_string = crawled_article_json['results'][0]['content']\n",
    "    soup = BeautifulSoup(html_string,'html.parser')\n",
    "    story_div = soup.find('div', id='storytext')\n",
    "    if story_div is None:\n",
    "      return None\n",
    "\n",
    "    article_text = story_div.get_text(strip=True, separator='\\n')\n",
    "\n",
    "    return article_text\n",
    "  except:\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0yxyz7i5b",
   "metadata": {},
   "source": [
    "## Article Text Extraction Function\n",
    "\n",
    "Define a function to extract the actual text content from individual article URLs. This function handles the full article scraping process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_article(category_url, batch_size = 10):\n",
    "  start_index = 1\n",
    "  while True:\n",
    "    crawled_page = crwal_url(f\"{category_url}?start={start_index}&count={batch_size}\")\n",
    "    crawled_page_json = json.loads(crawled_page.text)\n",
    "\n",
    "    if crawled_page_json['results'][0]['status_code'] != 200:\n",
    "      break\n",
    "\n",
    "    html_string = crawled_page_json['results'][0]['content']\n",
    "    soup = BeautifulSoup(html_string,'html.parser')\n",
    "\n",
    "\n",
    "    for article in soup.find_all('article'):\n",
    "      anchor_tag = article.find('a')\n",
    "      if anchor_tag is None:\n",
    "        continue\n",
    "      article_url = anchor_tag['href']\n",
    "      article_text = get_article_text(article_url)\n",
    "      if article_text is None:\n",
    "        continue\n",
    "\n",
    "      yield article_text\n",
    "    start_index += batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v0w782ptm6",
   "metadata": {},
   "source": [
    "## Article Iterator Function\n",
    "\n",
    "Create a generator function that iterates through all articles in a category with pagination support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed66e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "for news_category, category_url in urls_to_crawl.items():\n",
    "  print(f\"Crawling {news_category}\")\n",
    "  article_crawled_num = 0\n",
    "  for article_text in get_next_article(category_url):\n",
    "    data.append({'news_categoty' : news_category, 'article' : article_text})\n",
    "    article_crawled_num += 1\n",
    "    print(f\"Crawled {article_crawled_num} articles\")\n",
    "    if article_crawled_num >= 5:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8lcg3vsptub",
   "metadata": {},
   "source": [
    "## Main Scraping Loop\n",
    "\n",
    "Execute the main scraping process across all news categories. \n",
    "\n",
    "**TODO: When you hit the API limit, create a new DECODO account and re-run these cells**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fdcff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "csv_path = 'news_articles_Dataset.csv'\n",
    "if os.path.exists(csv_path):\n",
    "  data = pd.read_csv(csv_path).to_dict(orient='records')\n",
    "else:\n",
    "  data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ef718b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('news_articles_Dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etqzc7zft8",
   "metadata": {},
   "source": [
    "## Save Data to CSV\n",
    "\n",
    "Convert the collected article data into a pandas DataFrame and save it as a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5c3feca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_categoty</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Climate</td>\n",
       "      <td>Energy Secretary Chris Wright spearheaded a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Jeanine Pirro, the U.S. attorney for the Distr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>business</td>\n",
       "      <td>Circa 1750, The 'Spinning Jenny', invented by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Science</td>\n",
       "      <td>Klaus Vedfelt/Getty Images\\nFor a long time, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Health</td>\n",
       "      <td>Thomas_EyeDesign/iStockphoto/Getty Images\\nCre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_categoty                                            article\n",
       "20       Climate  Energy Secretary Chris Wright spearheaded a re...\n",
       "3       Politics  Jeanine Pirro, the U.S. attorney for the Distr...\n",
       "8       business  Circa 1750, The 'Spinning Jenny', invented by ...\n",
       "18       Science  Klaus Vedfelt/Getty Images\\nFor a long time, s...\n",
       "10        Health  Thomas_EyeDesign/iStockphoto/Getty Images\\nCre..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_csv = pd.read_csv('news_articles_Dataset.csv')\n",
    "head_csv.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f263e3",
   "metadata": {},
   "source": [
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7148cb",
   "metadata": {},
   "source": [
    "# LLM Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f68d3a",
   "metadata": {},
   "source": [
    "## Steps to fine tune an LLM\n",
    "### 1. Define Parameters\n",
    "### 2. Clean Dataset\n",
    "### 3. Wrnagle Dataset: Label, Train/Test Split, Vector Dataset and Conveartion\n",
    "### 4. Train the model\n",
    "### 5. Evaluate the model\n",
    "### 7. Model inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa765310",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "huggingface_hub.login(HF_LLM_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2aa6b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FILE_PATH = 'news_articles_Dataset.csv' \n",
    "TEXT_COLUMN = 'article'\n",
    "LABEL_COLUMN = 'news_categoty'\n",
    "TEST_SIZE = 0.2\n",
    "NUM_LABELS = max(2,df[LABEL_COLUMN].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abbcce",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff918f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleaner():\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def clean(self,text):\n",
    "    clean_text = self.remove_html(text)\n",
    "    clean_text = self.remove_spaces(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "  def remove_html(self,text):\n",
    "    clean_text = BeautifulSoup(text, 'html.parser').text\n",
    "    return clean_text\n",
    "  \n",
    "  def remove_spaces(self,text):\n",
    "    clean_text = re.sub(' +', '  ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82232ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_categoty</th>\n",
       "      <th>article</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Science</td>\n",
       "      <td>Three scientists learned they carry genes that...</td>\n",
       "      <td>Three  scientists  learned  they  carry  genes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Health</td>\n",
       "      <td>In this photo illustration, Pfizer-BioNTech CO...</td>\n",
       "      <td>In  this  photo  illustration,  Pfizer-BioNTec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Science</td>\n",
       "      <td>Fossils of the creature\\nSpicomellus\\nrevealed...</td>\n",
       "      <td>Fossils  of  the  creature\\nSpicomellus\\nrevea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Health</td>\n",
       "      <td>Health and Human Services Secretary Robert F. ...</td>\n",
       "      <td>Health  and  Human  Services  Secretary  Rober...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>business</td>\n",
       "      <td>President Donald Trump listens during a meetin...</td>\n",
       "      <td>President  Donald  Trump  listens  during  a  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_categoty                                            article  \\\n",
       "19       Science  Three scientists learned they carry genes that...   \n",
       "12        Health  In this photo illustration, Pfizer-BioNTech CO...   \n",
       "17       Science  Fossils of the creature\\nSpicomellus\\nrevealed...   \n",
       "11        Health  Health and Human Services Secretary Robert F. ...   \n",
       "9       business  President Donald Trump listens during a meetin...   \n",
       "\n",
       "                                         text_cleaned  \n",
       "19  Three  scientists  learned  they  carry  genes...  \n",
       "12  In  this  photo  illustration,  Pfizer-BioNTec...  \n",
       "17  Fossils  of  the  creature\\nSpicomellus\\nrevea...  \n",
       "11  Health  and  Human  Services  Secretary  Rober...  \n",
       "9   President  Donald  Trump  listens  during  a  ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner = Cleaner()\n",
    "df['text_cleaned'] = df[TEXT_COLUMN].apply(cleaner.clean)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de10ef5",
   "metadata": {},
   "source": [
    "## Wrangle The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bfb164b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_categoty</th>\n",
       "      <th>article</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>business</td>\n",
       "      <td>Circa 1750, The 'Spinning Jenny', invented by ...</td>\n",
       "      <td>Circa  1750,  The  'Spinning  Jenny',  invente...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politics</td>\n",
       "      <td>The entrance of the U.S. Department of Educati...</td>\n",
       "      <td>The  entrance  of  the  U.S.  Department  of  ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Health</td>\n",
       "      <td>In this photo illustration, Pfizer-BioNTech CO...</td>\n",
       "      <td>In  this  photo  illustration,  Pfizer-BioNTec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>business</td>\n",
       "      <td>President Donald Trump listens during a meetin...</td>\n",
       "      <td>President  Donald  Trump  listens  during  a  ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Health</td>\n",
       "      <td>NhuNgoc Pham with her family on the day she re...</td>\n",
       "      <td>NhuNgoc  Pham  with  her  family  on  the  day...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_categoty                                            article  \\\n",
       "8       business  Circa 1750, The 'Spinning Jenny', invented by ...   \n",
       "2       Politics  The entrance of the U.S. Department of Educati...   \n",
       "12        Health  In this photo illustration, Pfizer-BioNTech CO...   \n",
       "9       business  President Donald Trump listens during a meetin...   \n",
       "14        Health  NhuNgoc Pham with her family on the day she re...   \n",
       "\n",
       "                                         text_cleaned  label  \n",
       "8   Circa  1750,  The  'Spinning  Jenny',  invente...      4  \n",
       "2   The  entrance  of  the  U.S.  Department  of  ...      2  \n",
       "12  In  this  photo  illustration,  Pfizer-BioNTec...      1  \n",
       "9   President  Donald  Trump  listens  during  a  ...      4  \n",
       "14  NhuNgoc  Pham  with  her  family  on  the  day...      1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df[LABEL_COLUMN].tolist())\n",
    "df['label'] = le.transform(df[LABEL_COLUMN].tolist())\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29127c1",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1527400a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 4), (5, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test =  train_test_split(df, test_size=TEST_SIZE)\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af2b8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['text_cleaned','label']]\n",
    "df_test = df_test[['text_cleaned','label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22289960",
   "metadata": {},
   "source": [
    "# HF Convartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb848bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d90128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfd62ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "def preprocess_func(examples):\n",
    "  return tokenizer(examples['text_cleaned'],truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72fab3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 677.49 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 514.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokanized_train = train_dataset.map(preprocess_func, batched=True)\n",
    "tokanized_test = test_dataset.map(preprocess_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789ade6",
   "metadata": {},
   "source": [
    "# Model initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55a6a5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4671b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aff708e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model: 200\n"
     ]
    }
   ],
   "source": [
    "number_of_layers = 0\n",
    "for param in model.base_model.parameters():\n",
    "  number_of_layers += 1\n",
    "\n",
    "print(f\"Number of layers in the base model: {number_of_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81ff9a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model: 175\n"
     ]
    }
   ],
   "source": [
    "layer_num = 0\n",
    "for param in model.base_model.parameters():\n",
    "  if layer_num >= number_of_layers - 25:\n",
    "    break\n",
    "  layer_num += 1\n",
    "  param.requires_grad = False\n",
    "\n",
    "print(f\"Number of layers in the base model: {layer_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e6aabec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 4.20kB [00:00, 2.38MB/s]\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "  logits, labels = p\n",
    "  preds = np.argmax(logits, axis=-1)\n",
    "  return metric.compute(predictions=preds, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cad74e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=2000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokanized_train,\n",
    "    eval_dataset=tokanized_test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4537f7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 18:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=6.207253011067708, metrics={'train_runtime': 1149.8576, 'train_samples_per_second': 0.052, 'train_steps_per_second': 0.026, 'total_flos': 678123971100672.0, 'train_loss': 6.207253011067708, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e9794",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "baa09eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       2.0\n",
      "           2       0.00      0.00      0.00       2.0\n",
      "           3       0.00      0.00      0.00       0.0\n",
      "           4       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       5.0\n",
      "   macro avg       0.00      0.00      0.00       5.0\n",
      "weighted avg       0.00      0.00      0.00       5.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Daniel\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Daniel\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Daniel\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Daniel\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Daniel\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(tokanized_test)\n",
    "preds = np.argmax(preds[:3][0], axis=1)\n",
    "GT = df_test['label'].tolist()\n",
    "print(classification_report(GT, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406baef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
